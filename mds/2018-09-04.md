# 2018-09-04

标签（空格分隔）： CASIA

---

## Mac下OpenCV环境配置

- 安装brew
```#shell
$ ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
# 卸载
$  ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/uninstall)"
```
- 安装cmake
```
$ brew install cmake
```
- 安装wget
```
# 官方建议安装
$ brew install wget
```
- 安装opencv
```
$ brew install opencv
```
- Xcode配置
    1. Xcode新建一个基于Mac OS的Command Line Tool工程，语言选择C++；
    2. 在Targets->Build Settings里，我们要对**Header Search Path**和**Library Search Path**进行添加，以找到我们需要的库及头文件：
    ![hearder path][1]
    ![library path][2]
    3. 添加依赖库，接下来在工程的build pases里点击link binary with librarys的加号，在add other中按Command+shift+g，输入/usr/local/Cellar/opencv，在版本目录下的lib文件夹下，添加所有dylib文件，如图：
    ![phases][3]
    4. 接下来在工程中添加头文件：
    `#include <opencv2/opencv.hpp>`
    编译通过！完成！
- 我们来显示一张图片
```C++
#include <iostream>
#include <opencv2/opencv.hpp>

using namespace cv;

void help(char** argv ) {
    std::cout << "\n"
    << "A simple OpenCV program that loads and displays an image from disk\n"
    << argv[0] <<" <path/filename>\n"
    << "For example:\n"
    << argv[0] << " ../fruits.jpg\n"
    << std::endl;
}


int main( int argc, char** argv ) {
    
    std::cout << "argc:" << argc << std::endl;
    std::cout << "argv:" << argv[0] << std::endl;
    
    
    if (argc != 2) {
        help(argv);
        return 0;
    }
    
    cv::Mat img = cv::imread( argv[1], -1 );
    
    if( img.empty() ) return -1;
    
    cv::namedWindow( "Example 2-1", cv::WINDOW_AUTOSIZE );
    cv::imshow( "Example 2-1", img);
    cv::waitKey( 0 );
    cv::destroyWindow( "Example 2-1" );
    
    return 0;
}
```
以上示例使用opencv从内存中加载并显示了一张图片：
![结果显示][4]

## 概念拾遗

[光栅化](https://www.jianshu.com/p/54fe91a946e2)

[图形渲染管线](https://www.zhihu.com/question/29163054)


## 翻译

### 1.介绍

基于点云的三维物体检测是各种现实世界应用的重要组成部分，如自主导航\[11,14]，家政机器人\[26]和增强/虚拟现实\[27]。与基于图像的检测相比，LiDAR提供了可靠的深度信息，可用于精确定位对象并表征其形状\[21,5]。然而，与图像不同，由于诸如3D空间的非均匀采样，传感器的有效范围，遮挡和相对姿势等因素，LiDAR点云是稀疏的并且具有高度可变的点密度。为了处理这些挑战，许多方法都是手动制作针对3D对象检测进行调整的点云的特征表示。有几种方法将点云投射到一个透视视图并应用基于图像的特征提取技术\[28,15,22]。其他方法将点云光栅化为3D体素网格，并使用手工制作的特征对每个体素进行编码\[41,9,37,38,21,5]。但是，这些手动设计选择会引入信息瓶颈，阻碍这些方法在检测任务中有效地利用3D形状信息和所需的不变性。图像识别\[20]和检测\[13]任务的重大突破是由于从手工制作的特征转变为机器学习特征。

最近，Qi等人\[29]提出了PointNet，一种端到端深度神经网络，可直接从点云中学习逐点特征。这种方法在三维物体识别，三维物体部分分割和逐点语义分割任务上展示了令人印象深刻的结果。在\[30]中，引入了改进版的PointNet，使网络能够学习不同规模的局部结构。为了获得满意的结果，这两种方法在所有输入点（~1k点）上训练了特征变压器网络。由于使用LiDAR获得的典型点云包含~100k点，因此在\[29,30]中训练架构会导致高计算和内存要求。将3D特征学习网络扩展到数量级更多的点以及3D检测任务是我们在本文中提出的主要挑战。
区域提议网络（RPN）\[32]是一种高度优化的有效物体检测算法\[17,1,5,34]。然而，这种方法要求数据密集并且以张量结构（例如图像，视频）组织，这不是典型的LiDAR点云的情况。在本文中，我们缩小了点集特征学习与用于3D检测任务的RPN之间的差距。

我们介绍了VoxelNet，这是一种通用的3D检测框架，可以同时学习点云的判别特征表示，并以端到端的方式预测精确的3D边界框，如图2所示。

![Figure 2][5]

我们设计了一种新颖的体素特征编码（VFE）层，通过将逐点特征与局部聚合特征相结合，实现体素内的点间交互。堆叠多个VFE层允许学习复杂的特征来表征局部3D形状信息。具体而言，VoxelNet将点云划分为等间距的3D体素，通过堆叠的VFE层对每个体素进行编码，然后3D卷积进一步聚合局部体素特征，将点云转换为高维体积表示。最后，RPN消耗了体积表示并产生检测结果。这种有效的算法既有利于稀疏点结构，也有利于体素网格上的高效并行处理。
我们根据KITTI基准\[11]提供的鸟瞰图检测和全3D检测任务评估VoxelNet。实验结果表明，VoxelNet在很大程度上优于最先进的基于LiDAR的3D检测方法。我们还证明了Voxel-Net在检测LiDAR点云中的行人和骑车者方面取得了非常令人鼓舞的成果。

## 


--------
  
\[1] P. Bariya and K. Nishino. Scale-hierarchical 3d object recog- nition in cluttered scenes. In 2010 IEEE Computer Soci- ety Conference on Computer Vision and Pattern Recognition, pages 1657–1664, 2010. 2
\[2] L. Bo, X. Ren, and D. Fox. Depth Kernel Descriptors for Object Recognition. In IROS, September 2011. 2
\[3] X.Chen,K.Kundu,Z.Zhang,H.Ma,S.Fidler,andR.Urta- sun. Monocular 3d object detection for autonomous driving. In IEEE CVPR, 2016. 2, 5, 6, 7
\[4] X.Chen,K.Kundu,Y.Zhu,A.Berneshawi,H.Ma,S.Fidler, and R. Urtasun. 3d object proposals for accurate object class detection. In NIPS, 2015. 2, 5, 6, 7
\[5] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia. Multi-view 3d object detection network for autonomous driving. In IEEE CVPR,2017. 1,2,3,4,5,6,7,8
\[6] C. Choi, Y. Taguchi, O. Tuzel, M. Y. Liu, and S. Rama- lingam. Voting-based pose estimation for robotic assembly using a 3d sensor. In 2012 IEEE International Conference on Robotics and Automation, pages 1724–1731, 2012. 2
\[7] C. S. Chua and R. Jarvis. Point signatures: A new repre- sentation for 3d object recognition. International Journal of Computer Vision, 25(1):63–85, Oct 1997. 2
\[8] C.DoraiandA.K.Jain.Cosmos-arepresentationschemefor 3d free-form objects. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(10):1115–1130, 1997. 2
\[9] M.Engelcke,D.Rao,D.Z.Wang,C.H.Tong,andI.Posner. Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks. In 2017 IEEE In- ternational Conference on Robotics and Automation (ICRA), pages 1355–1361, May 2017. 1, 2
\[10] M. Enzweiler and D. M. Gavrila. A multilevel mixture-of- experts framework for pedestrian classification. IEEE Trans- actions on Image Processing, 20(10):2967–2979, Oct 2011. 3
\[11] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for au- tonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012. 1, 2, 5, 6
\[12] R. Girshick. Fast r-cnn. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV ’15, 2015. 5, 6
\[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea- ture hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580–587, 2014. 1
\[14] R. Gomez-Ojeda, J. Briales, and J. Gonzalez-Jimenez. Pl- svo: Semi-direct monocular visual odometry by combining points and line segments. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 4211–4216, Oct 2016. 1
\[15] A. Gonzalez, G. Villalonga, J. Xu, D. Vazquez, J. Amores, and A. Lopez. Multiview random forest of local experts com- bining rgb and lidar data for pedestrian detection. In IEEE Intelligent Vehicles Symposium (IV), 2015. 1, 2
\[16] A. Gonzlez, D. Vzquez, A. M. Lpez, and J. Amores. On- board object detection: Multicue, multimodal, and multiview random forest of local experts. IEEE Transactions on Cyber- netics, 47(11):3980–3990, Nov 2017. 3
\[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), pages 770– 778, June 2016. 2, 6
\[18] A. G. Howard. Some improvements on deep convolu- tional neural network based image classification. CoRR, abs/1312.5402, 2013. 6
\[19] A.E.JohnsonandM.Hebert.Usingspinimagesforefficient object recognition in cluttered 3d scenes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 21(5):433– 449, 1999. 2
\[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097–1105. Curran Associates, Inc., 2012. 1, 6
\[21] B. Li. 3d fully convolutional network for vehicle detection in point cloud. In IROS, 2017. 1, 2, 5, 7
\[22] B. Li, T. Zhang, and T. Xia. Vehicle detection from 3d lidar using fully convolutional network. In Robotics: Science and Systems, 2016. 1, 2, 5, 7
\[23] T. Lin, P. Goyal, R. B. Girshick, K. He, and P. Dolla ́r. Focal loss for dense object detection. IEEE ICCV, 2017. 4
\[24] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg. Ssd: Single shot multibox detector. In ECCV, pages 21–37, 2016. 2
\[25] A. Mian, M. Bennamoun, and R. Owens. On the repeata- bility and quality of keypoints for local feature-based 3d ob- ject retrieval from cluttered scenes. International Journal of Computer Vision, 89(2):348–361, Sep 2010. 2
\[26] Y.-J. Oh and Y. Watanabe. Development of small robot for home floor cleaning. In Proceedings of the 41st SICE Annual Conference. SICE 2002., volume 5, pages 3222–3223 vol.5, Aug 2002. 1
\[27] Y. Park, V. Lepetit, and W. Woo. Multiple 3d object tracking for augmented reality. In 2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality, pages 117– 120, Sept 2008. 1
\[28] C. Premebida, J. Carreira, J. Batista, and U. Nunes. Pedes- trian detection combining RGB and dense LIDAR data. In IROS, pages 0–1. IEEE, Sep 2014. 1, 2
\[29] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. 1
\[30] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. arXiv preprint arXiv:1706.02413, 2017. 1
\[31] J. Redmon and A. Farhadi. YOLO9000: better, faster, stronger. In IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR), 2017. 2
\[32] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To- wards real-time object detection with region proposal net- works. In Advances in Neural Information Processing Sys- tems 28, pages 91–99. 2015. 2, 3, 4, 5
\[33] R. B. Rusu, N. Blodow, and M. Beetz. Fast point feature histograms (fpfh) for 3d registration. In 2009 IEEE Interna- tional Conference on Robotics and Automation, pages 3212– 3217, 2009. 2
\[34] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finoc- chio, R. Moore, A. Kipman, and A. Blake. Real-time human pose recognition in parts from single depth images. In CVPR 2011, pages 1297–1304, 2011. 2
\[35] K. Simonyan and A. Zisserman. Very deep convolu- tional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. 6
\[36] S.SongandM.Chandraker.Jointsfmanddetectioncuesfor monocular 3d localization in road scenes. In IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR), pages 3734–3742, June 2015. 2
\[37] S.SongandJ.Xiao.Slidingshapesfor3dobjectdetectionin depth images. In European Conference on Computer Vision, Proceedings, pages 634–651, Cham, 2014. Springer Interna- tional Publishing. 1, 2
\[38] S. Song and J. Xiao. Deep Sliding Shapes for amodal 3D object detection in RGB-D images. In CVPR, 2016. 1, 2, 4, 5
\[39] F. Stein and G. Medioni. Structural indexing: efficient 3-d object recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(2):125–145, 1992. 2
\[40] O.Tuzel,M.-Y.Liu,Y.Taguchi,andA.Raghunathan.Learn- ing to rank 3d features. In 13th European Conference on Computer Vision, Proceedings, Part I, pages 520–535, 2014. 2
\[41] D. Z. Wang and I. Posner. Voting for voting in online point cloud object detection. In Proceedings of Robotics: Science and Systems, Rome, Italy, July 2015. 1, 2
\[42] Y. Xiang, W. Choi, Y. Lin, and S. Savarese. Data-driven 3d voxel patterns for object category recognition. In Pro- ceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, 2015. 2
\[43] M. Z. Zia, M. Stark, B. Schiele, and K. Schindler. De- tailed 3d representations for object recognition and model- ing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(11):2608–2623, 2013. 2
\[44] M. Z. Zia, M. Stark, and K. Schindler. Are cars just 3d boxes? jointly estimating the 3d shape of multiple objects. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, pages 3678–3685, June 2014. 2


  [1]: http://static.zybuluo.com/usiege/ijn7pvpb4ia1ij4garfmpx5h/image_1cmh6a3r16m5q5cjtjjdmfh095.png
  [2]: http://static.zybuluo.com/usiege/pvh9odcbxgi7avwqgx9fl37f/image_1cmh5l35itll1sjj1k03dk79216.png
  [3]: http://static.zybuluo.com/usiege/mqxb0lv50qv9p55175kvkh9g/image_1cmh65lp2n3fqnd1ma34521bb18o.png
  [4]: http://static.zybuluo.com/usiege/uia19hu9g3e2ejfntw7jxt79/image_1cmh8ebfb189v1mi2hp6cj01u6ia1.png
  [5]: http://static.zybuluo.com/usiege/jvoqxwkqjzw63ucddue25cjh/image_1cmi8r8o2rnt1p8re4g1r77ipv9.png